# SMB Lead Marketplace

**TurboTax for SMB Data Intake** â€“ A web app where SMBs walk through a chat-driven "interview" + doc upload, and you output a clean, structured dossier that can be sold as a lead or used to prefill other systems.

## Product Overview

This platform serves two verticals:
- **Commercial Insurance (SMB P&C)** â€“ GL/BOP/WC for restaurants, retail shops, and local services
- **SMB Lending** â€“ Term loans and lines of credit for working capital

Both verticals share ~70% of the same "business profile" fields, making the data model reusable.

### Primary Personas

1. **SMB Owner** â€“ Confused but motivated, wants funding/insurance quickly, hates forms
2. **Receiver / Buyer** â€“ Broker or lender who wants "underwriting-ready" leads with normalized data

## Setup

### 1. Git prerequisites
- Ensure you have a GitHub account and access to the repository
- Clone the repo:
  ```bash
  git clone <repo-url>
  cd insurance-app
  ```
- Work from the `main` branch or create a feature branch

### 2. Requirements & packages
- **Node.js** v18+
- **pnpm** v8+ (`npm install -g pnpm` if you don't have it)
- **Docker Desktop** (for Postgres, Redis, MinIO, Temporal services)
- Optional but recommended:
  - Temporal CLI (if you want to run worker workflows end-to-end)
  - Prisma CLI (`pnpm dlx prisma`)

After installing prerequisites, install dependencies once:
```bash
pnpm install
```

### 3. Environment Variables

Create a `.env` file in `apps/api` with:
```env
DATABASE_URL="postgresql://dev:dev@localhost:5432/app"
OPENAI_API_KEY="your-openai-api-key"  # Optional - chat works without it using rule-based patterns
OPENAI_MODEL="gpt-4o-mini"  # Optional, only used if OPENAI_API_KEY is set
API_URL="http://localhost:4000"
MINIO_ENDPOINT="localhost"
MINIO_PORT="9000"
MINIO_ACCESS_KEY="dev"
MINIO_SECRET_KEY="dev12345"
```

**Note:** The chat system uses rule-based pattern matching (like the main branch) and works **without** an OpenAI API key. If you provide an API key, it will enhance responses with more natural language, but it's completely optional.

### 4. Commands to run the app locally

From the repo root:
```bash
# start supporting services (Postgres, Redis, MinIO, Temporal)
docker compose up -d

# apply database migrations (run whenever schema changes)
cd apps/api
npx prisma migrate dev
pnpm run seed          # seeds default data/field definitions
cd ../..

# start services (run each time in separate terminals)
pnpm api               # NestJS API at http://localhost:4000
pnpm worker            # Temporal worker (for doc processing)
pnpm web               # Next.js web app at http://localhost:3000
```

Stop services with `Ctrl+C`. To shut down the infrastructure containers, run `docker compose down`.

### 5. Where to view the app
- **Web app:** http://localhost:3000
- **API docs:** http://localhost:4000/docs (Swagger UI)
- **Database UI (optional):** `cd apps/api && pnpm prisma:studio`

## Repository overview

```
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ api/          # NestJS backend (REST API, Prisma, chat workflow logic)
â”‚   â”œâ”€â”€ web/          # Next.js frontend (SMB intake, partner portal, admin tools)
â”‚   â””â”€â”€ worker/       # Temporal worker for document processing/extractions
â”œâ”€â”€ packages/         # Shared libraries (UI components, config)
â”œâ”€â”€ compose.yaml      # Docker services (Postgres, Redis, MinIO, Temporal)
â””â”€â”€ README.md         # This document
```

Key directories inside `apps/api`:
- `prisma/schema.prisma` â€“ Prisma models for sessions, leads, partners, documents, etc.
- `src/sessions` â€“ Session management (replaces old submissions)
- `src/leads` â€“ Lead management and completion tracking
- `src/chat` â€“ LLM-powered chat orchestration
- `src/partners` â€“ Partner/buyer management and lead matching
- `src/documents` â€“ Document upload and processing

Key directories inside `apps/web`:
- `src/app` â€“ Next.js App Router pages
  - `/` â€“ Landing page with two CTAs (insurance/lending)
  - `/start` â€“ Pre-intake form
  - `/intake/[sessionId]` â€“ Chat-driven intake flow
  - `/review/[sessionId]` â€“ Summary & consent
  - `/partners/dashboard` â€“ Partner lead dashboard
  - `/partners/leads/[leadId]` â€“ Lead detail view

## Pages overview

### SMB-facing pages
- `/` â€“ Landing page with two CTAs: "I want better insurance quotes" and "I want small business funding"
- `/start` â€“ Pre-intake form (collects vertical, business type, name, email)
- `/intake/[sessionId]` â€“ Main chat interface with document upload panel
- `/review/[sessionId]` â€“ Summary page with confirmation and matching trigger

### Partner/Buyer-facing pages
- `/partners/dashboard` â€“ Lead table showing assigned leads
- `/partners/leads/[leadId]` â€“ Full lead detail view with accept/reject actions
- `/partners/settings/appetite` â€“ Appetite configuration (TODO)

### Admin pages (TODO)
- `/admin/login` â€“ Admin authentication
- `/admin/leads` â€“ All leads management
- `/admin/partners` â€“ Partner management
- `/admin/analytics` â€“ Analytics dashboard

## Key tools & platform functionality

### Prisma ORM
Central data layer for sessions, leads, chat messages, documents, extracted fields, partners, and lead assignments. The schema in `apps/api/prisma/schema.prisma` drives migrations and type-safe access throughout the NestJS API.

### Document ingestion & OCR
Uploads are stored in MinIO (S3-compatible). The Temporal worker (`apps/worker`) orchestrates downloading files, running OCR/text extraction, and field extraction. PDFs are read via pdf-parse; the pipeline creates chunks, classifies document types, and writes extracted fields back to the API.

### AI-powered chat orchestration
The chat service (`apps/api/src/chat/chat.service.ts`) uses OpenAI to:
- Conduct natural conversations with SMB owners
- Extract structured field data from chat messages
- Ask follow-up questions based on vertical and business type
- Update lead records in real-time

### Field extraction pipeline
Documents are processed by the Temporal worker which:
- Downloads files from MinIO
- Extracts text from PDFs
- Classifies document types
- Extracts fields using regex patterns and keyword matching
- Saves extracted fields to the database with confidence scores

### Lead matching
The matching service (`apps/api/src/partners/matching.service.ts`) implements rule-based matching:
- Matches leads to partners based on appetite criteria (states, industries, revenue ranges, etc.)
- Creates `LeadAssignment` records when matches are found
- Updates lead status to `ASSIGNED`

### Temporal workflow integration
The worker communicates with the API via HTTP to trigger document processing, ensuring long-running tasks (OCR, extractions) are resilient. Temporal's activity definitions live in `apps/worker/src/activities.ts`.

## Data Model

### Core Models
- **Session** â€“ Represents an SMB intake session (vertical, business type, owner info)
- **Lead** â€“ Complete business profile with all collected fields (shared + vertical-specific)
- **ConversationMessage** â€“ Chat messages with optional field updates
- **Document** â€“ Uploaded documents with processing status
- **ExtractedField** â€“ Fields extracted from documents
- **FieldCandidate** â€“ Field candidates from chat or documents (before confirmation)
- **Partner** â€“ Broker/lender with appetite configuration
- **LeadAssignment** â€“ Assignment of leads to partners

## API Endpoints

### Sessions
- `POST /sessions` â€“ Create new session
- `GET /sessions/:id` â€“ Get session with lead and messages
- `GET /sessions` â€“ List all sessions

### Chat
- `POST /chat/:sessionId` â€“ Send chat message (returns assistant response + field updates)

### Leads
- `GET /leads/:id` â€“ Get full lead detail
- `POST /leads/:id/confirm` â€“ Mark lead as ready for matching
- `GET /leads` â€“ List all leads

### Partners
- `POST /partners` â€“ Create partner
- `GET /partners/:id` â€“ Get partner with assignments
- `GET /partners/:id/leads` â€“ Get leads assigned to partner
- `POST /partners/:id/leads/:leadId/accept` â€“ Accept a lead
- `POST /partners/:id/leads/:leadId/reject` â€“ Reject a lead
- `POST /partners/match/:leadId` â€“ Run matching logic

### Documents
- `POST /documents/upload` â€“ Create document record after upload
- `GET /documents/:id` â€“ Get document with extracted fields
- `PATCH /documents/:id` â€“ Update document processing status

## Handoff notes

- **Environment variables:** See `.env.example` or team documentation; ensure Next.js (`apps/web`) and API (`apps/api`) have matching `NEXT_PUBLIC_API_URL` and `DATABASE_URL` settings.
- **Database migrations:** Run `npx prisma migrate dev` after pulling new migrations. Seed with `pnpm run seed` inside `apps/api`.
- **Temporal workflows:** `pnpm worker` uses Temporal; keep the service running if testing document extraction end-to-end. Make sure Temporal server is running (`docker compose up -d temporal`).
- **LLM Integration:** The chat system uses rule-based pattern matching by default (no API key needed). Set `OPENAI_API_KEY` in environment to optionally enhance responses with more natural language, but it's not required.
- **Linting/testing:** `pnpm lint` and `pnpm test` (per workspace) before committing.
- **Deployment:** Production deployment scripts TBD.

For questions or handoffs, document context in PR descriptions and update this README as new pages/services are added. Happy shipping! ðŸš€
